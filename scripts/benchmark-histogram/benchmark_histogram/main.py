import dataclasses
import json
import logging
import pathlib
import re
import typing

import click
import matplotlib.cm as cm
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns


@click.command("benchmark_histogram")
@click.argument("sources", type=click.Path(exists=True), nargs=-1)
@click.option("--commits", type=str, required=False)
def main(sources: typing.List[str], commits: typing.Optional[str]):
    # Load all benchmarks:
    benchmarks = Benchmark.load_all(*[pathlib.Path(source) for source in sources])

    # If --commits was specified, sort the benchmarks by their commits:
    if commits is not None:
        commit_order = [commit[0:8] for commit in commits.split(",")]
        benchmarks.sort(
            key=lambda benchmark: (
                benchmark.commit_index(commit_order),
                benchmark.platform,
                benchmark.ghc_version,
            )
        )
    else:
        benchmarks.sort(
            key=lambda benchmark: (
                benchmark.commit_hash,
                benchmark.platform,
                benchmark.ghc_version,
            )
        )

    # Build a pandas DataFrame:
    data = {
        "mean": [benchmark.result.mean for benchmark in benchmarks],
        "commit_hash": [benchmark.commit_hash for benchmark in benchmarks],
        "platform": [benchmark.platform for benchmark in benchmarks],
        "ghc_version": [benchmark.ghc_version for benchmark in benchmarks],
    }
    del data["platform"]  # Remove "platform" as it isn't relevant

    sns.set()  # Set seaborn defaults
    df = pd.DataFrame(data, columns=data.keys())
    df = df.set_index(["commit_hash", "ghc_version"])
    ag = df["mean"].unstack().plot(kind="bar", stacked=False)
    ag.tick_params("x", labelsize=10, rotation=True)
    ag.set_ylabel("Mean Time in Second[s]")
    ag.set_xlabel("Commit Hash")
    plt.show()


@dataclasses.dataclass
class BenchmarkResult:
    command: str
    mean: float
    stddev: float
    median: float
    user: float
    system: float
    min: float
    max: float
    times: typing.List[float]
    exit_codes: typing.List[int]

    def __len__(self) -> int:
        return len(self.times)


@dataclasses.dataclass
class Benchmark:
    commit_hash: str
    ghc_version: str
    platform: str
    result: BenchmarkResult

    def commit_index(self, commit_order: typing.List[str]) -> int:
        try:
            return commit_order.index(self.commit_hash)
        except ValueError:
            raise ValueError(f"Could not find {self.commit_hash} in --commits.")

    # Regular expression which matches the commands generated by scripts/benchmark:
    re_command: typing.ClassVar[re.Pattern] = re.compile(
        r"vehicle-(?P<commit_hash>[0-9a-f]{8})-(?P<platform>macOS|Linux|Windows)-ghc-(?P<ghc_version>[0-9.]+)"
    )

    @classmethod
    def parse_command(cls, command: str) -> typing.Dict[str, str]:
        match = cls.re_command.match(command)
        if match is None:
            raise ValueError(
                " ".join(
                    [
                        f"Expected command to be of the form",
                        "'vehicle-<commit_hash>-<platform>-ghc-<ghc_version>',",
                        "found: {command}",
                    ]
                )
            )
        return match.groupdict() if match is not None else None

    @classmethod
    def load_all(cls, *paths: pathlib.Path) -> typing.List["Benchmark"]:
        benchmarks: typing.List[Benchmark] = []

        for path in paths:
            # Load the results from the benchmark file:
            try:
                benchmark_results = json.loads(path.read_bytes())
                # Process the results:
                for n, benchmark_result in enumerate(
                    benchmark_results.get("results", [])
                ):
                    try:
                        benchmark_result = BenchmarkResult(**benchmark_result)
                        benchmark_info = cls.parse_command(benchmark_result.command)
                        benchmarks.append(
                            Benchmark(**benchmark_info, result=benchmark_result)
                        )
                    except Exception as e:
                        logging.warning(f"Skipping {path} {n}: {e}")
            except Exception as e:
                logging.warning(f"Skipping {path}: {e}")

        # Return all benchmarks:
        return benchmarks
